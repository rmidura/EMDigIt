{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Description :\n",
        "### **Part 2 of the EM_Digit Processing Project: Automated Validation, Correction, and Data Integrity Checks**  \n",
        "\n",
        "Part 2 of the **EM_Digit Processing Project** is dedicated to **ensuring the accuracy, consistency, and reliability of location data** through a series of automated validation and correction processes. This phase addresses potential errors in geographic coordinates, distance calculations, and state assignments, leveraging **geospatial computation techniques and automated heuristics** to detect and resolve inconsistencies efficiently. The process begins with **two key distance validation checks**: `dist_test1`, which flags locations where the revised distance from the previous known location exceeds **30 km**, and `dist_test2`, which identifies locations where the actual recorded coordinates deviate by more than **30 km from the approximated coordinates**. These tests help detect data anomalies such as misplaced locations or miscalculated distances.\n",
        "\n",
        " Additionally, the **coordinate trend test (`coords_test`)** is applied to identify instances where the direction of movement **unexpectedly reverses**, ensuring logical continuity in the travel path. **State validation tests (`state_test` and `state_test2`)** further enhance data accuracy by checking for discrepancies in state assignments—`state_test` flags locations where the **state differs from both the previous and next locations**, while `state_test2` identifies entries where the **state does not match any other instance within the same route description**, helping maintain **geopolitical consistency** in the dataset. To further automate corrections, the script conducts **route boundary adjustments**, where missing latitude and longitude values are **filled using the nearest valid entry within the same route**, ensuring a **continuous geographic flow**. If a **bearing (direction of travel) is missing or incorrect**, it is **automatically recalculated using GeographicLib’s Geodesic function**, preventing erroneous trajectory assumptions. A key component of this phase is the **alternative location matching process**, which leverages **geodesic distance calculations** to find the **nearest possible match from the gazetteer dataset (`gaz_df`)**, providing an alternative geographic reference for locations with incomplete or incorrect data. Throughout this stage, all corrections are **logged in `bounds_check_log.csv`**, ensuring **transparency and traceability** of modifications.\n",
        "\n",
        "\n",
        " Finally, an **`Automated_Flag`** column is added to mark any row flagged by these validation tests, and the **cleaned, validated dataset** is exported as `full_test_df.csv` for further review and analysis. By the end of Part 2, the dataset has undergone **a rigorous automated verification process**, significantly reducing the need for manual intervention while ensuring that only **high-quality, geospatially consistent data** proceeds to the next stage. This phase **sets the foundation for further integration, visualization, and mapping**, making the dataset **ready for advanced geographic and statistical analyses** in the subsequent stages of the project. 🚀"
      ],
      "metadata": {
        "id": "daJvvFaV3_Se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Prelims\n",
        "\n",
        "Do not forget to type in the itinerary_name accordingly"
      ],
      "metadata": {
        "id": "OpBMjV9JuXQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6sQSXoPwsnYY"
      },
      "outputs": [],
      "source": [
        "#Packages\n",
        "import os\n",
        "import csv as csv\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import math\n",
        "#! pip install geographiclib\n",
        "from geographiclib.geodesic import Geodesic\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#File import/export currently written for mounted Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "metadata": {
        "id": "yT9jtu8qukBj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Corrected directory path\n",
        "df = pd.read_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/clean_df.csv')\n",
        "\n",
        "# Ensure missing values are handled correctly\n",
        "df.fillna('na', inplace=True)\n",
        "\n",
        "# Rename column if needed\n",
        "if \"region_type\" in df.columns:\n",
        "    df.rename(columns={\"region_type\": \"line_type\"}, inplace=True)\n",
        "\n",
        "# Display the first few rows to verify correctness\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "QiRLIhD1un6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establishing route boundaries using bounds_df\n"
      ],
      "metadata": {
        "id": "p0cY0FESu2pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geographiclib.geodesic import Geodesic\n",
        "\n",
        "def get_route_boundaries(df):\n",
        "    \"\"\"Extracts route boundaries (first and last location) and computes bearing for each route.\"\"\"\n",
        "\n",
        "    # Filter rows where 'line_type' is 'location' and 'geonameId' is not NaN\n",
        "    filtered_df = df[df['line_type'] == 'location'].dropna(subset=['geonameId'])\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        print(\"Warning: No valid locations found in the dataset.\")\n",
        "        return pd.DataFrame(columns=['route_description', 'first_lat', 'last_lat', 'first_lng', 'last_lng', 'bearing'])\n",
        "\n",
        "    # Group by 'route_description' and extract first/last coordinates\n",
        "    route_boundaries = (\n",
        "        filtered_df.groupby('route_description')\n",
        "        .agg(\n",
        "            first_lat=('Location_Lat', 'first'),\n",
        "            last_lat=('Location_Lat', 'last'),\n",
        "            first_lng=('Location_Lng', 'first'),\n",
        "            last_lng=('Location_Lng', 'last')\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Compute bearing using Geodesic calculations\n",
        "    geod = Geodesic.WGS84\n",
        "    def compute_bearing(row):\n",
        "        if pd.isna(row['first_lat']) or pd.isna(row['first_lng']) or pd.isna(row['last_lat']) or pd.isna(row['last_lng']):\n",
        "            return np.nan\n",
        "        return geod.Inverse(row['first_lat'], row['first_lng'], row['last_lat'], row['last_lng'])['azi1']\n",
        "\n",
        "    # Apply bearing calculation\n",
        "    route_boundaries['bearing'] = route_boundaries.apply(compute_bearing, axis=1)\n",
        "\n",
        "    return route_boundaries\n",
        "\n",
        "# Generate route boundaries\n",
        "bounds_df = get_route_boundaries(df)\n",
        "\n",
        "# Export to CSV with correct file path\n",
        "bounds_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/bounds_df.csv', index=False)\n",
        "\n",
        "# Display DataFrame\n",
        "bounds_df.head()\n"
      ],
      "metadata": {
        "id": "Ow22H7JWv6up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automated error checking for bounds - Under review - tends to be fool proof most of the time\n"
      ],
      "metadata": {
        "id": "Lfr990uiweHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geographiclib.geodesic import Geodesic\n",
        "\n",
        "# Load the existing bounds file\n",
        "bounds_df = pd.read_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/bounds_df.csv')\n",
        "\n",
        "# Initialize a log to track corrections\n",
        "correction_log = []\n",
        "\n",
        "# Geodesic calculator\n",
        "geod = Geodesic.WGS84\n",
        "\n",
        "# Function to recompute bearing\n",
        "def compute_bearing(row):\n",
        "    \"\"\"Compute bearing if coordinates are valid.\"\"\"\n",
        "    if pd.isna(row['first_lat']) or pd.isna(row['first_lng']) or pd.isna(row['last_lat']) or pd.isna(row['last_lng']):\n",
        "        return np.nan\n",
        "    return geod.Inverse(row['first_lat'], row['first_lng'], row['last_lat'], row['last_lng'])['azi1']\n",
        "\n",
        "# Iterate over the dataset for corrections\n",
        "for index, row in bounds_df.iterrows():\n",
        "    corrected = False\n",
        "\n",
        "    # If first_lat or first_lng is missing, try using next valid row within the same route\n",
        "    if pd.isna(row['first_lat']) or pd.isna(row['first_lng']):\n",
        "        next_valid = bounds_df.loc[(bounds_df['route_description'] == row['route_description']) &\n",
        "                                   bounds_df['first_lat'].notna()].head(1)\n",
        "        if not next_valid.empty:\n",
        "            bounds_df.at[index, 'first_lat'] = next_valid.iloc[0]['first_lat']\n",
        "            bounds_df.at[index, 'first_lng'] = next_valid.iloc[0]['first_lng']\n",
        "            correction_log.append([row['route_description'], 'first_lat/lng corrected using next valid row'])\n",
        "            corrected = True\n",
        "\n",
        "    # If last_lat or last_lng is missing, try using previous valid row within the same route\n",
        "    if pd.isna(row['last_lat']) or pd.isna(row['last_lng']):\n",
        "        prev_valid = bounds_df.loc[(bounds_df['route_description'] == row['route_description']) &\n",
        "                                   bounds_df['last_lat'].notna()].tail(1)\n",
        "        if not prev_valid.empty:\n",
        "            bounds_df.at[index, 'last_lat'] = prev_valid.iloc[0]['last_lat']\n",
        "            bounds_df.at[index, 'last_lng'] = prev_valid.iloc[0]['last_lng']\n",
        "            correction_log.append([row['route_description'], 'last_lat/lng corrected using previous valid row'])\n",
        "            corrected = True\n",
        "\n",
        "    # Check if bearing is missing or incorrect, recompute it\n",
        "    if pd.isna(row['bearing']) or row['bearing'] < 0 or row['bearing'] > 360:\n",
        "        new_bearing = compute_bearing(row)\n",
        "        if not pd.isna(new_bearing):\n",
        "            bounds_df.at[index, 'bearing'] = new_bearing\n",
        "            correction_log.append([row['route_description'], 'Bearing recalculated'])\n",
        "            corrected = True\n",
        "\n",
        "# Save the corrected file only if changes were made\n",
        "if correction_log:\n",
        "    bounds_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/bounds_df.csv', index=False)\n",
        "\n",
        "    # Save the correction log\n",
        "    log_df = pd.DataFrame(correction_log, columns=['route_description', 'Correction Applied'])\n",
        "    log_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/bounds_check_log.csv', index=False)\n",
        "\n",
        "    print(\"✅ Bounds file corrected and saved. Corrections logged in bounds_check_log.csv.\")\n",
        "else:\n",
        "    print(\"✅ No corrections needed. The bounds file is clean.\")\n"
      ],
      "metadata": {
        "id": "K3OjnNXDwdx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automated Distance and Unit Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "MzUyYjnIyOH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# ============================== STEP 1: Extract Unit Measurements ==============================\n",
        "\n",
        "# Dictionary mapping abbreviations to units\n",
        "unit_mapping = {\n",
        "    '[Pp]': 'posts', 'Poste': 'posts', '[Mm]': 'miles',\n",
        "    '[Mm]iglia': 'miles', '[Ll]eghe': 'leagues', '[Ll]': 'leagues'\n",
        "}\n",
        "\n",
        "# Function to extract numeric values based on unit abbreviations\n",
        "def extract_units(text, mapping):\n",
        "    \"\"\"Extracts distance values based on unit abbreviations in the content column.\"\"\"\n",
        "    result = {'miles': np.nan, 'posts': np.nan, 'leagues': np.nan}\n",
        "    for abbr, unit in mapping.items():\n",
        "        match = re.search(rf'{abbr}\\.?(\\d+)', text)\n",
        "        if match:\n",
        "            result[unit] = int(match.group(1))\n",
        "    return pd.Series(result)\n",
        "\n",
        "# Apply the function to create new columns\n",
        "df[['miles', 'posts', 'leagues']] = df['content'].apply(lambda x: extract_units(str(x), unit_mapping))\n",
        "\n",
        "# ============================== STEP 2: Fill Missing Coordinates Based on \"qui\" ==============================\n",
        "\n",
        "def fill_missing_coordinates(df):\n",
        "    \"\"\"\n",
        "    If 'qui' is detected in \"prose\" content, fill missing Location_Lat and Location_Lng\n",
        "    using the nearest previous valid coordinates.\n",
        "    \"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        if row['line_type'] == 'prose' and re.search(r'qui\\b', str(row['content'])):\n",
        "            prev_index = index - 1\n",
        "            while prev_index >= 0:\n",
        "                if pd.notnull(df.at[prev_index, 'Location_Lat']) and pd.notnull(df.at[prev_index, 'Location_Lng']):\n",
        "                    df.at[index, 'Location_Lat'] = df.at[prev_index, 'Location_Lat']\n",
        "                    df.at[index, 'Location_Lng'] = df.at[prev_index, 'Location_Lng']\n",
        "                    break\n",
        "                prev_index -= 1\n",
        "\n",
        "# Apply the function\n",
        "fill_missing_coordinates(df)\n",
        "\n",
        "# ============================== STEP 3: Interpolate Missing Coordinates ==============================\n",
        "\n",
        "def interpolate_coordinates(df):\n",
        "    \"\"\"\n",
        "    For missing coordinates in 'prose' rows, interpolate values by averaging\n",
        "    the nearest valid previous and next locations.\n",
        "    \"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        if row['line_type'] == \"prose\" and pd.isna(row['leagues']) and pd.isna(row['posts']) and pd.isna(row['miles']):\n",
        "            prev_index, next_index = index - 1, index + 1\n",
        "\n",
        "            # Find previous valid location\n",
        "            while prev_index >= 0:\n",
        "                if df.at[prev_index, 'route_description'] == row['route_description'] and \\\n",
        "                        not (pd.isna(df.at[prev_index, 'leagues']) and pd.isna(df.at[prev_index, 'posts']) and pd.isna(df.at[prev_index, 'miles'])):\n",
        "                    break\n",
        "                prev_index -= 1\n",
        "\n",
        "            # Find next valid location\n",
        "            while next_index < len(df):\n",
        "                if df.at[next_index, 'route_description'] == row['route_description'] and \\\n",
        "                        not (pd.isna(df.at[next_index, 'leagues']) and pd.isna(df.at[next_index, 'posts']) and pd.isna(df.at[next_index, 'miles'])):\n",
        "                    break\n",
        "                next_index += 1\n",
        "\n",
        "            # Interpolate coordinates if both previous and next valid locations exist\n",
        "            if prev_index >= 0 and next_index < len(df):\n",
        "                df.at[index, 'Location_Lat'] = (df.at[prev_index, 'Location_Lat'] + df.at[next_index, 'Location_Lat']) / 2\n",
        "                df.at[index, 'Location_Lng'] = (df.at[prev_index, 'Location_Lng'] + df.at[next_index, 'Location_Lng']) / 2\n",
        "\n",
        "# Apply interpolation\n",
        "interpolate_coordinates(df)\n",
        "\n",
        "# ============================== STEP 4: Assign Distance & Unit ==============================\n",
        "\n",
        "def find_distance_and_unit(row):\n",
        "    \"\"\"\n",
        "    Determines the left-most non-zero value for distance & assigns the corresponding unit.\n",
        "    \"\"\"\n",
        "    for col in ['leagues', 'posts', 'miles']:\n",
        "        if pd.notna(row[col]) and row[col] > 0:\n",
        "            return row[col], col\n",
        "    return np.nan, np.nan\n",
        "\n",
        "# Apply the function to each row\n",
        "df[['distance', 'unit']] = df.apply(find_distance_and_unit, axis=1, result_type='expand')\n",
        "\n",
        "# ============================== STEP 5: Convert Distances to Kilometers ==============================\n",
        "\n",
        "# Distance conversion dictionary based on contemporary state & country values\n",
        "dist_conversion = {\n",
        "    'Carinthia': {'leagues': 7.20, 'miles': 1.58, 'posts': 14.09},\n",
        "    'Lower Austria': {'leagues': 7.51, 'miles': 1.49, 'posts': 19.37},\n",
        "    'Salzburg': {'leagues': 8.89, 'miles': 2.50, 'posts': 14.90},\n",
        "    'Styria': {'leagues': 9.15, 'miles': 1.32, 'posts': 17.91},\n",
        "    'AT': {'leagues': 6.60, 'miles': 1.49, 'posts': 15.02},  # Austria fallback\n",
        "    'Berlin': {'leagues': 7.44, 'miles': 1.81, 'posts': 15.90},\n",
        "    'Bavaria': {'leagues': 7.43, 'miles': 1.81, 'posts': 15.80},\n",
        "    'DE': {'leagues': 7.44, 'miles': 1.81, 'posts': 15.90},  # Germany fallback\n",
        "    'England': {'leagues': 5.68, 'miles': 1.65, 'posts': 11.11},\n",
        "    'GB': {'leagues': 5.68, 'miles': 1.65, 'posts': 11.11},  # UK fallback\n",
        "}\n",
        "\n",
        "# Function to convert historical distances to kilometers\n",
        "def convert_distance_to_km(row):\n",
        "    \"\"\"\n",
        "    Converts historical distance measurements into kilometers using contemporary conversion values.\n",
        "    \"\"\"\n",
        "    if pd.isna(row['distance']) or pd.isna(row['unit']):\n",
        "        return np.nan\n",
        "\n",
        "    state = row['state']\n",
        "    country = row['country_code']\n",
        "\n",
        "    # Look for state-specific conversion first, else fallback to country\n",
        "    conversion_factor = dist_conversion.get(state, dist_conversion.get(country, {})).get(row['unit'], np.nan)\n",
        "\n",
        "    return row['distance'] * conversion_factor if pd.notna(conversion_factor) else np.nan\n",
        "\n",
        "# Apply conversion\n",
        "df['distance_km'] = df.apply(convert_distance_to_km, axis=1)\n",
        "\n",
        "# ============================== STEP 6: Save Final Processed Data ==============================\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/processed_distances.csv', index=False)\n",
        "\n",
        "print(\"✅ Data processing complete. File saved as 'processed_distances.csv'\")\n"
      ],
      "metadata": {
        "id": "3f6M5aG7zD1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Usage: Convert distance using the dictionary\n",
        "state = \"Lombardy\"\n",
        "unit = \"miles\"\n",
        "distance_value = 10  # Example distance in miles\n",
        "\n",
        "# Lookup conversion factor\n",
        "conversion_factor = dist_conversion.get(state, dist_conversion.get(\"IT\", {})).get(unit, np.nan)\n",
        "\n",
        "# Convert to kilometers\n",
        "distance_in_km = distance_value * conversion_factor if pd.notna(conversion_factor) else np.nan\n",
        "\n",
        "print(f\"Converted Distance: {distance_in_km} km\")\n"
      ],
      "metadata": {
        "id": "avtWri8Ozg1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================== Distance Conversion Dictionary ==============================\n",
        "# Uses approximate distance conversion from contemporary measures by state and country_code\n",
        "# to calculate distance in modern kilometers.\n",
        "# The state value is used if present; if not, defaults to country_code.\n",
        "\n",
        "dist_conversion = {\n",
        "    'Carinthia': {'leagues': 7.20, 'miles': 1.58, 'posts': 14.09},\n",
        "    'Lower Austria': {'leagues': 7.51, 'miles': 1.49, 'posts': 19.37},\n",
        "    'Salzburg': {'leagues': 8.89, 'miles': 2.50, 'posts': 14.90},\n",
        "    'Styria': {'leagues': 9.15, 'miles': 1.32, 'posts': 17.91},\n",
        "    'Tyrol': {'leagues': 5.03, 'miles': 1.21, 'posts': 13.42},\n",
        "    'Upper Austria': {'leagues': 8.04, 'miles': 1.49, 'posts': 18.56},\n",
        "    'Vienna': {'leagues': 6.49, 'miles': 1.49, 'posts': 14.58},\n",
        "    'Vorarlberg': {'leagues': 6.60, 'miles': 1.49, 'posts': 10.80},\n",
        "    'AT': {'leagues': 6.60, 'miles': 1.49, 'posts': 15.02},  # Austria fallback\n",
        "\n",
        "    'Brussels Capital': {'leagues': 5.78, 'miles': 1.25, 'posts': 12.46},\n",
        "    'Flanders': {'leagues': 5.37, 'miles': 1.25, 'posts': 12.90},\n",
        "    'Wallonia': {'leagues': 6.21, 'miles': 1.25, 'posts': 13.30},\n",
        "    'BE': {'leagues': 5.88, 'miles': 1.25, 'posts': 13.14},  # Belgium fallback\n",
        "\n",
        "    'Baden-Wurttemberg': {'leagues': 8.17, 'miles': 1.81, 'posts': 16.77},\n",
        "    'Bavaria': {'leagues': 7.43, 'miles': 1.81, 'posts': 15.80},\n",
        "    'Berlin': {'leagues': 7.44, 'miles': 1.81, 'posts': 15.90},\n",
        "    'DE': {'leagues': 7.44, 'miles': 1.81, 'posts': 15.90},  # Germany fallback\n",
        "\n",
        "    'England': {'leagues': 5.68, 'miles': 1.65, 'posts': 11.11},\n",
        "    'GB': {'leagues': 5.68, 'miles': 1.65, 'posts': 11.11},  # UK fallback\n",
        "\n",
        "    'Madrid': {'leagues': 5.38, 'miles': 1.65, 'posts': 12.67},\n",
        "    'Navarre': {'leagues': 5.02, 'miles': 1.65, 'posts': 17.33},\n",
        "    'Valencia': {'leagues': 6.09, 'miles': 1.65, 'posts': 13.41},\n",
        "    'ES': {'leagues': 5.70, 'miles': 1.65, 'posts': 12.47},  # Spain fallback\n",
        "\n",
        "    'Lombardy': {'leagues': 5.17, 'miles': 1.58, 'posts': 13.14},\n",
        "    'Tuscany': {'leagues': 5.17, 'miles': 1.74, 'posts': 13.60},\n",
        "    'Veneto': {'leagues': 5.17, 'miles': 1.62, 'posts': 13.51},\n",
        "    'IT': {'leagues': 5.17, 'miles': 1.66, 'posts': 12.77},  # Italy fallback\n",
        "\n",
        "    'Warsaw': {'leagues': 6.00, 'miles': 1.50, 'posts': 12.00},\n",
        "    'PL': {'leagues': 5.68, 'miles': 1.65, 'posts': 10.48},  # Poland fallback\n",
        "\n",
        "    'Lisbon': {'leagues': 4.66, 'miles': 1.65, 'posts': 13.36},\n",
        "    'Porto': {'leagues': 5.00, 'miles': 1.60, 'posts': 14.50},\n",
        "    'PT': {'leagues': 5.72, 'miles': 1.65, 'posts': 14.50},  # Portugal fallback\n",
        "\n",
        "    'Ljubljana': {'leagues': 5.68, 'miles': 1.65, 'posts': 9.60},\n",
        "    'Maribor': {'leagues': 5.68, 'miles': 1.65, 'posts': 17.96},\n",
        "    'SI': {'leagues': 8.82, 'miles': 1.65, 'posts': 17.64},  # Slovenia fallback\n",
        "}\n",
        "\n",
        "# ✅ Distance conversion dictionary successfully stored for further calculations.\n"
      ],
      "metadata": {
        "id": "tHbpVHWpzZcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================== Function to Calculate Revised Distance ==============================\n",
        "\n",
        "def calculate_revised_distance(row):\n",
        "    \"\"\"\n",
        "    Converts historical distance values into modern kilometers using state-based conversion factors.\n",
        "    If state-level data is unavailable, defaults to country_code.\n",
        "    \"\"\"\n",
        "    state = row.get('state')\n",
        "    country_code = row.get('country_code')\n",
        "    distance = str(row.get('distance', ''))\n",
        "    unit = row.get('unit')\n",
        "\n",
        "    # Skip rows with missing values\n",
        "    if pd.isna(distance) or pd.isna(unit) or pd.isna(country_code):\n",
        "        return np.nan\n",
        "\n",
        "    # Extract first valid numeric distance if multiple values exist\n",
        "    try:\n",
        "        distance_values = distance.split('|')\n",
        "        distance_float = float(distance_values[0].strip())\n",
        "    except (ValueError, IndexError):\n",
        "        print(f\"⚠️ Warning: Invalid distance value detected - {distance}\")\n",
        "        return np.nan\n",
        "\n",
        "    # Use state if available; otherwise, default to country_code\n",
        "    conversion_key = state if pd.notna(state) else country_code\n",
        "\n",
        "    if conversion_key in dist and unit in dist[conversion_key]:\n",
        "        # Get conversion ratio\n",
        "        ratio = dist[conversion_key].get(unit)\n",
        "\n",
        "        # Ensure ratio is a valid number\n",
        "        try:\n",
        "            ratio_float = float(ratio)\n",
        "        except (ValueError, TypeError):\n",
        "            print(f\"⚠️ Warning: Invalid conversion ratio detected for {conversion_key} - {ratio}\")\n",
        "            return np.nan\n",
        "\n",
        "        # Calculate revised distance\n",
        "        return round(distance_float * ratio_float, 2)  # Round to 2 decimal places\n",
        "\n",
        "    return np.nan  # If no conversion found, return NaN\n",
        "\n",
        "# ============================== Apply Function & Save to CSV ==============================\n",
        "\n",
        "df['revised_distance'] = df.apply(calculate_revised_distance, axis=1)\n",
        "\n",
        "# Save the updated dataframe\n",
        "output_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/distance_extract.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"✅ Revised distance calculation complete. File saved at: {output_path}\")\n"
      ],
      "metadata": {
        "id": "tgxf2HwWzk5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establishing Bearing\n"
      ],
      "metadata": {
        "id": "f4Gvnw9yz_Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from geographiclib.geodesic import Geodesic\n",
        "\n",
        "def calculate_bearing(df):\n",
        "    \"\"\"\n",
        "    Computes the bearing (direction) between consecutive valid locations.\n",
        "    Uses vectorized operations instead of row-wise iteration.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize Bearing column with NaN\n",
        "    df['Bearing'] = np.nan\n",
        "\n",
        "    # Filter rows where line_type is \"location\" or \"prose\"\n",
        "    valid_rows = df[df['line_type'].isin(['location', 'prose']) & df['Location_Lat'].notna() & df['Location_Lng'].notna()]\n",
        "\n",
        "    if valid_rows.empty:\n",
        "        print(\"⚠️ Warning: No valid locations found for bearing calculation.\")\n",
        "        return df\n",
        "\n",
        "    # Shift latitude and longitude values to get preceding and following locations\n",
        "    df['prev_Lat'] = df['Location_Lat'].shift(1)\n",
        "    df['prev_Lng'] = df['Location_Lng'].shift(1)\n",
        "    df['next_Lat'] = df['Location_Lat'].shift(-1)\n",
        "    df['next_Lng'] = df['Location_Lng'].shift(-1)\n",
        "\n",
        "    # Function to compute bearing using GeographicLib\n",
        "    def compute_bearing(lat1, lng1, lat2, lng2):\n",
        "        if pd.isna(lat1) or pd.isna(lng1) or pd.isna(lat2) or pd.isna(lng2):\n",
        "            return np.nan\n",
        "        geod = Geodesic.WGS84\n",
        "        return geod.Inverse(lat1, lng1, lat2, lng2)['azi1']\n",
        "\n",
        "    # Apply the function to calculate bearing\n",
        "    df['Bearing'] = df.apply(lambda row: compute_bearing(row['prev_Lat'], row['prev_Lng'], row['next_Lat'], row['next_Lng']), axis=1)\n",
        "\n",
        "    # Drop helper columns\n",
        "    df.drop(columns=['prev_Lat', 'prev_Lng', 'next_Lat', 'next_Lng'], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply the optimized bearing calculation\n",
        "df = calculate_bearing(df)\n"
      ],
      "metadata": {
        "id": "6VRrvJ_Yz9Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establishing approximate coordinates\n",
        "\n",
        "Uses bearing and revised_distance to approximate coordinates of a given location"
      ],
      "metadata": {
        "id": "6YPm-y2r0c_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "from geographiclib.geodesic import Geodesic\n",
        "\n",
        "def calculate_approx_coordinates(df):\n",
        "    \"\"\"\n",
        "    Computes approximate coordinates for each row using bearing and distance from the last valid location.\n",
        "    \"\"\"\n",
        "\n",
        "    df_copy = df.copy()\n",
        "    df_copy.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Initialize Approximate Coordinates column\n",
        "    if 'approx_coordinates' not in df_copy:\n",
        "        df_copy['approx_coordinates'] = np.nan\n",
        "\n",
        "    geod = Geodesic.WGS84  # Geodesic model for accurate distance calculations\n",
        "\n",
        "    # Filter rows with valid route, bearing, and distance\n",
        "    valid_rows = df_copy[df_copy[['Bearing', 'revised_distance', 'route_description']].notna().all(axis=1)]\n",
        "\n",
        "    if valid_rows.empty:\n",
        "        print(\"⚠️ Warning: No valid data found for coordinate calculation.\")\n",
        "        return df_copy\n",
        "\n",
        "    # Shift valid locations for calculations\n",
        "    df_copy['prev_Lat'] = df_copy['Location_Lat'].shift(1)\n",
        "    df_copy['prev_Lng'] = df_copy['Location_Lng'].shift(1)\n",
        "\n",
        "    # Function to compute new coordinates\n",
        "    def compute_new_coords(lat1, lng1, bearing, distance_km):\n",
        "        if pd.isna(lat1) or pd.isna(lng1) or pd.isna(bearing) or pd.isna(distance_km):\n",
        "            return np.nan\n",
        "        g = geod.Direct(lat1, lng1, bearing, distance_km * 1000)  # Convert km to meters\n",
        "        return (round(g['lat2'], 6), round(g['lon2'], 6))  # Round to 6 decimal places\n",
        "\n",
        "    # Apply function to compute new coordinates\n",
        "    df_copy['approx_coordinates'] = df_copy.apply(\n",
        "        lambda row: compute_new_coords(row['prev_Lat'], row['prev_Lng'], row['Bearing'], row['revised_distance']), axis=1\n",
        "    )\n",
        "\n",
        "    # Convert tuples to strings\n",
        "    df_copy['approx_coordinates'] = df_copy['approx_coordinates'].astype(str).str.strip('()')\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "# Apply coordinate calculation\n",
        "df = calculate_approx_coordinates(df)\n",
        "\n",
        "# ============================== Interpolation for Prose Entries ==============================\n",
        "\n",
        "def interpolate_prose_coordinates(df):\n",
        "    \"\"\"\n",
        "    If prose entries have no distance or unit, their coordinates are set as the midpoint between the nearest valid locations.\n",
        "    \"\"\"\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if row['line_type'] == \"prose\" and pd.isna(row['unit']) and pd.isna(row['revised_distance']):\n",
        "            prev_index, next_index = index - 1, index + 1\n",
        "\n",
        "            # Find previous valid location\n",
        "            while prev_index >= 0:\n",
        "                if df.at[prev_index, 'route_description'] == row['route_description'] and \\\n",
        "                        not (pd.isna(df.at[prev_index, 'unit']) and pd.isna(df.at[prev_index, 'revised_distance'])):\n",
        "                    break\n",
        "                prev_index -= 1\n",
        "\n",
        "            # Find next valid location\n",
        "            while next_index < len(df):\n",
        "                if df.at[next_index, 'route_description'] == row['route_description'] and \\\n",
        "                        not (pd.isna(df.at[next_index, 'unit']) and pd.isna(df.at[next_index, 'revised_distance'])):\n",
        "                    break\n",
        "                next_index += 1\n",
        "\n",
        "            # Calculate coordinates if both previous and next valid locations exist\n",
        "            if prev_index >= 0 and next_index < len(df):\n",
        "                prev_lat, prev_lng = df.at[prev_index, 'Location_Lat'], df.at[prev_index, 'Location_Lng']\n",
        "                next_lat, next_lng = df.at[next_index, 'Location_Lat'], df.at[next_index, 'Location_Lng']\n",
        "\n",
        "                df.at[index, 'Location_Lat'] = (prev_lat + next_lat) / 2\n",
        "                df.at[index, 'Location_Lng'] = (prev_lng + next_lng) / 2\n",
        "\n",
        "# Apply interpolation for prose coordinates\n",
        "interpolate_prose_coordinates(df)\n",
        "\n",
        "# Save processed data\n",
        "output_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/processed_coordinates.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"✅ Coordinate processing complete. File saved at: {output_path}\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from geographiclib.geodesic import Geodesic\n",
        "\n",
        "def calculate_approx_coordinates(df):\n",
        "    \"\"\"\n",
        "    Computes approximate coordinates for each row using bearing and distance from the last valid location.\n",
        "    \"\"\"\n",
        "\n",
        "    df_copy = df.copy()\n",
        "    df_copy.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Initialize Approximate Coordinates column\n",
        "    if 'approx_coordinates' not in df_copy:\n",
        "        df_copy['approx_coordinates'] = np.nan\n",
        "\n",
        "    geod = Geodesic.WGS84  # Geodesic model for accurate distance calculations\n",
        "\n",
        "    # Filter rows with valid route, bearing, and distance\n",
        "    valid_rows = df_copy[df_copy[['Bearing', 'revised_distance', 'route_description']].notna().all(axis=1)]\n",
        "\n",
        "    if valid_rows.empty:\n",
        "        print(\"⚠️ Warning: No valid data found for coordinate calculation.\")\n",
        "        return df_copy\n",
        "\n",
        "    # Shift valid locations for calculations\n",
        "    df_copy['prev_Lat'] = df_copy['Location_Lat'].shift(1)\n",
        "    df_copy['prev_Lng'] = df_copy['Location_Lng'].shift(1)\n",
        "\n",
        "    # Function to compute new coordinates\n",
        "    def compute_new_coords(lat1, lng1, bearing, distance_km):\n",
        "        if pd.isna(lat1) or pd.isna(lng1) or pd.isna(bearing) or pd.isna(distance_km):\n",
        "            return np.nan\n",
        "        g = geod.Direct(lat1, lng1, bearing, distance_km * 1000)  # Convert km to meters\n",
        "        return (round(g['lat2'], 6), round(g['lon2'], 6))  # Round to 6 decimal places\n",
        "\n",
        "    # Apply function to compute new coordinates\n",
        "    df_copy['approx_coordinates'] = df_copy.apply(\n",
        "        lambda row: compute_new_coords(row['prev_Lat'], row['prev_Lng'], row['Bearing'], row['revised_distance']), axis=1\n",
        "    )\n",
        "\n",
        "    # Convert tuples to strings\n",
        "    df_copy['approx_coordinates'] = df_copy['approx_coordinates'].astype(str).str.strip('()')\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "# Apply coordinate calculation\n",
        "df = calculate_approx_coordinates(df)\n",
        "\n",
        "# ============================== Interpolation for Prose Entries ==============================\n",
        "\n",
        "def interpolate_prose_coordinates(df):\n",
        "    \"\"\"\n",
        "    If prose entries have no distance or unit, their coordinates are set as the midpoint between the nearest valid locations.\n",
        "    \"\"\"\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if row['line_type'] == \"prose\" and pd.isna(row['unit']) and pd.isna(row['revised_distance']):\n",
        "            prev_index, next_index = index - 1, index + 1\n",
        "\n",
        "            # Find previous valid location\n",
        "            while prev_index >= 0:\n",
        "                if df.at[prev_index, 'route_description'] == row['route_description'] and \\\n",
        "                        not (pd.isna(df.at[prev_index, 'unit']) and pd.isna(df.at[prev_index, 'revised_distance'])):\n",
        "                    break\n",
        "                prev_index -= 1\n",
        "\n",
        "            # Find next valid location\n",
        "            while next_index < len(df):\n",
        "                if df.at[next_index, 'route_description'] == row['route_description'] and \\\n",
        "                        not (pd.isna(df.at[next_index, 'unit']) and pd.isna(df.at[next_index, 'revised_distance'])):\n",
        "                    break\n",
        "                next_index += 1\n",
        "\n",
        "            # Calculate coordinates if both previous and next valid locations exist\n",
        "            if prev_index >= 0 and next_index < len(df):\n",
        "                prev_lat, prev_lng = df.at[prev_index, 'Location_Lat'], df.at[prev_index, 'Location_Lng']\n",
        "                next_lat, next_lng = df.at[next_index, 'Location_Lat'], df.at[next_index, 'Location_Lng']\n",
        "\n",
        "                df.at[index, 'Location_Lat'] = (prev_lat + next_lat) / 2\n",
        "                df.at[index, 'Location_Lng'] = (prev_lng + next_lng) / 2\n",
        "\n",
        "# Apply interpolation for prose coordinates\n",
        "interpolate_prose_coordinates(df)\n",
        "\n",
        "# Save processed data\n",
        "output_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/processed_coordinates.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"✅ Coordinate processing complete. File saved at: {output_path}\")\n"
      ],
      "metadata": {
        "id": "HkpUAron0jPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching. to alternative locations"
      ],
      "metadata": {
        "id": "mihcSv8f1wTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.distance import geodesic\n",
        "\n",
        "# ============================== Function to Compute Geodesic Distance ==============================\n",
        "\n",
        "def geodesic_distance(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"Computes geodesic distance (in kilometers) between two coordinate points.\"\"\"\n",
        "    if pd.isna(lat1) or pd.isna(lon1) or pd.isna(lat2) or pd.isna(lon2):\n",
        "        return np.nan\n",
        "    return geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
        "\n",
        "# ============================== Function to Find Closest Gazetteer Match ==============================\n",
        "\n",
        "def find_closest_match(approx_df, gaz_df):\n",
        "    \"\"\"\n",
        "    Finds the closest gazetteer match for each row in approx_df based on geodesic distance.\n",
        "    Returns a DataFrame with alternative matched locations and distances.\n",
        "    \"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Ensure gazetteer has valid lat/lng data\n",
        "    gaz_df = gaz_df.dropna(subset=['Location_Lat', 'Location_Lng'])\n",
        "\n",
        "    for index, approx_row in approx_df.iterrows():\n",
        "        approx_coordinates = approx_row.get(\"approx_coordinates\", None)\n",
        "        location_lat = approx_row.get(\"Location_Lat\", None)\n",
        "        location_lon = approx_row.get(\"Location_Lng\", None)\n",
        "\n",
        "        if pd.notna(approx_coordinates) and ',' in approx_coordinates and pd.isna(location_lat):\n",
        "            try:\n",
        "                approx_lat, approx_lon = map(float, approx_coordinates.split(\",\"))\n",
        "            except ValueError:\n",
        "                continue  # Skip invalid coordinate parsing\n",
        "\n",
        "            # Compute distances for all gazetteer entries at once\n",
        "            gaz_df[\"distance_km\"] = gaz_df.apply(\n",
        "                lambda row: geodesic_distance(approx_lat, approx_lon, row[\"Location_Lat\"], row[\"Location_Lng\"]), axis=1\n",
        "            )\n",
        "\n",
        "            # Find the closest location\n",
        "            closest_match = gaz_df.loc[gaz_df[\"distance_km\"].idxmin()]\n",
        "\n",
        "            results.append({\n",
        "                \"approx_coordinates\": approx_coordinates,\n",
        "                \"alternative_location\": (closest_match[\"Location_Lat\"], closest_match[\"Location_Lng\"]),\n",
        "                \"alt_distance\": closest_match[\"distance_km\"],\n",
        "                \"alternative_match\": closest_match[\"id\"],\n",
        "                \"alternative_geoname\": closest_match[\"geoname\"]\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# ============================== Import Data & Find Closest Matches ==============================\n",
        "\n",
        "# Import gazetteer data\n",
        "gaz_df = pd.read_csv(\"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/gazetteer.csv\")\n",
        "\n",
        "# Use df as the approximation dataset\n",
        "approx_df = df\n",
        "\n",
        "# Find closest matches from the gazetteer\n",
        "closest_match_df = find_closest_match(approx_df, gaz_df)\n",
        "\n",
        "# ============================== Merge Approximate & Gazetteer Data ==============================\n",
        "\n",
        "# Merge to include alternative matches from gazetteer\n",
        "merged_df = pd.merge(approx_df, closest_match_df, how='left', suffixes=('_x', '_y'))\n",
        "merged_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Save the final merged dataset\n",
        "output_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/merged_df.csv\"\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "\n",
        "# Update df with merged results\n",
        "df = merged_df\n",
        "\n",
        "print(f\"✅ Closest match processing complete. File saved at: {output_path}\")\n"
      ],
      "metadata": {
        "id": "DKEbbacO1wFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distancd tests. - combined"
      ],
      "metadata": {
        "id": "Jrtc7n8d2Ml3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "# ============================== Optimized Distance Test ==============================\n",
        "\n",
        "def calculate_distance_tests(row):\n",
        "    \"\"\"\n",
        "    Computes two distance tests:\n",
        "    1. dist_test1: True if revised_distance > 30 km from the previous row.\n",
        "    2. dist_test2: True if actual coordinates are more than 30 km from the approximated coordinates.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get current coordinates\n",
        "    actual_coords = (row['Location_Lat'], row['Location_Lng'])\n",
        "\n",
        "    # Test 1: Distance from previous row\n",
        "    prev_index = row.name - 1\n",
        "    if prev_index >= 0:\n",
        "        prev_row = df.iloc[prev_index]\n",
        "        prev_coords = (prev_row['Location_Lat'], prev_row['Location_Lng'])\n",
        "        dist_test1 = (\n",
        "            pd.notna(row['revised_distance']) and row['revised_distance'] > 30\n",
        "        ) or (\n",
        "            pd.notna(prev_coords[0]) and pd.notna(prev_coords[1]) and\n",
        "            pd.notna(actual_coords[0]) and pd.notna(actual_coords[1]) and\n",
        "            geodesic(prev_coords, actual_coords).kilometers > 30\n",
        "        )\n",
        "    else:\n",
        "        dist_test1 = False\n",
        "\n",
        "    # Test 2: Distance from approximated coordinates\n",
        "    approx_coords_str = row['approx_coordinates']\n",
        "    if pd.notna(approx_coords_str) and approx_coords_str != 'None' and ',' in approx_coords_str:\n",
        "        try:\n",
        "            approx_coords = tuple(map(float, approx_coords_str.strip('()').split(',')))\n",
        "            dist_test2 = (\n",
        "                pd.notna(actual_coords[0]) and pd.notna(actual_coords[1]) and\n",
        "                geodesic(approx_coords, actual_coords).kilometers > 30\n",
        "            )\n",
        "        except ValueError:\n",
        "            dist_test2 = False\n",
        "    else:\n",
        "        dist_test2 = False\n",
        "\n",
        "    return pd.Series([dist_test1, dist_test2])\n",
        "\n",
        "# Apply the combined distance tests\n",
        "df[['dist_test1', 'dist_test2']] = df.apply(calculate_distance_tests, axis=1)\n",
        "\n",
        "# ============================== Export Processed Data ==============================\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/full_test_df.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\" Distance tests completed. File saved at: {output_path}\")\n"
      ],
      "metadata": {
        "id": "S-IXUGGa2r88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coordinate Testing - scarapped but uselful, fully optimised - always room for improvement\n",
        "\n"
      ],
      "metadata": {
        "id": "B4IJsMIr2_JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ============================== Optimized Coordinate Trend Test ==============================\n",
        "\n",
        "def check_coords_trend(df):\n",
        "    \"\"\"\n",
        "    Flags locations where the latitude and longitude shift from increasing to decreasing (or vice versa).\n",
        "    \"\"\"\n",
        "\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Filter only relevant rows\n",
        "    location_df = df_copy[(df_copy['line_type'] == \"location\") & df_copy['Location_Lat'].notna() & df_copy['Location_Lng'].notna()]\n",
        "\n",
        "    # Ensure correct row order\n",
        "    location_df = location_df.sort_index()\n",
        "\n",
        "    # Compute latitude and longitude changes\n",
        "    location_df['lat_diff'] = location_df['Location_Lat'].diff()\n",
        "    location_df['lng_diff'] = location_df['Location_Lng'].diff()\n",
        "\n",
        "    # Shift to compare previous and next trends\n",
        "    location_df['lat_diff_shifted'] = location_df['lat_diff'].shift(-1)\n",
        "    location_df['lng_diff_shifted'] = location_df['lng_diff'].shift(-1)\n",
        "\n",
        "    # Identify rows where both latitude and longitude reverse trends\n",
        "    location_df['coords_test'] = (location_df['lat_diff'] * location_df['lat_diff_shifted'] < 0) & \\\n",
        "                                 (location_df['lng_diff'] * location_df['lng_diff_shifted'] < 0)\n",
        "\n",
        "    # Merge back into the original dataframe\n",
        "    df_copy = df_copy.merge(location_df[['coords_test']], how=\"left\", left_index=True, right_index=True)\n",
        "\n",
        "    return df_copy.fillna(False)\n",
        "\n",
        "df = check_coords_trend(df)\n",
        "\n",
        "# ============================== Optimized State Consistency Tests ==============================\n",
        "\n",
        "def check_state_test(df):\n",
        "    \"\"\"\n",
        "    Flags locations where the state changes compared to the previous or next location.\n",
        "    \"\"\"\n",
        "\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Ensure 'state_test' column exists\n",
        "    df_copy['state_test'] = False\n",
        "\n",
        "    # Filter relevant rows\n",
        "    location_rows = df_copy[df_copy['line_type'] == 'location'].index\n",
        "\n",
        "    for index in location_rows:\n",
        "        state_n = df_copy.at[index, 'state']\n",
        "\n",
        "        if pd.isna(state_n) or state_n == '':\n",
        "            continue  # Skip missing values\n",
        "\n",
        "        prev_index, next_index = index - 1, index + 1\n",
        "        prev_state = df_copy.at[prev_index, 'state'] if prev_index in df_copy.index else None\n",
        "        next_state = df_copy.at[next_index, 'state'] if next_index in df_copy.index else None\n",
        "\n",
        "        # Flag if state changes from both the previous and next location\n",
        "        if state_n != prev_state and (next_state is None or state_n != next_state):\n",
        "            df_copy.at[index, 'state_test'] = True\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "df = check_state_test(df)\n",
        "\n",
        "# ============================== Optimized Route-Wide State Check ==============================\n",
        "\n",
        "def check_state_test2(df):\n",
        "    \"\"\"\n",
        "    Flags locations where the state does not match any state occurring within the same route.\n",
        "    \"\"\"\n",
        "\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Ensure 'state_test2' column exists\n",
        "    df_copy['state_test2'] = False\n",
        "\n",
        "    # Get all unique states per route\n",
        "    route_state_map = df_copy.groupby('route_description')['state'].apply(lambda x: set(x.dropna()))\n",
        "\n",
        "    # Apply test\n",
        "    df_copy['state_test2'] = df_copy.apply(\n",
        "        lambda row: row['state'] not in route_state_map.get(row['route_description'], set()) if pd.notna(row['state']) else False,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "df = check_state_test2(df)\n",
        "\n",
        "# ============================== Automated Flagging & Export ==============================\n",
        "\n",
        "# Ensure the test columns exist and fill missing values with False\n",
        "test_columns = ['dist_test1', 'dist_test2', 'coords_test', 'state_test', 'state_test2']\n",
        "for col in test_columns:\n",
        "    if col not in df.columns:\n",
        "        df[col] = False\n",
        "    df[col] = df[col].fillna(False)\n",
        "\n",
        "# Add Automated Flagging\n",
        "df['Automated_Flag'] = df[test_columns].any(axis=1)\n",
        "\n",
        "# Save the processed file\n",
        "output_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/full_test_df.csv\"\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"✅ Coordinate and State Tests completed. File saved at: {output_path}\")\n"
      ],
      "metadata": {
        "id": "KI7iufIf2-k8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}