{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "EmDigit Processing Notebook - New version\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1SFMDAv1RrBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Input the unique id name for the itinerary composed of author initials (SA in case of no known author), date of publication (SD in case of no known date)\n",
        "#and distinguishing letter in case of multiple editions of same year (i.e. \"GH1563A\")\n",
        "itinerary_name = \"GM1684\""
      ],
      "metadata": {
        "id": "MrE4upLkR7q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installations to be run once before executing the notebook\n"
      ],
      "metadata": {
        "id": "QMV3hKCGR74s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installs (run once)\n",
        "!pip install fuzzywuzzy\n",
        "!pip install python-Levenshtein\n",
        "!pip install -U gspread-dataframe\n",
        "!pip install numpy\n",
        "\n",
        "#Packages\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import csv as csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.core.arrays.interval import NA\n",
        "import re\n",
        "import string\n",
        "from fuzzywuzzy import fuzz"
      ],
      "metadata": {
        "id": "YwH_RQGUlPca",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting google drive so that it has access to Transkribus XML files"
      ],
      "metadata": {
        "id": "70PPi7oeUTrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t1ltO4onUS8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting parameters for the data"
      ],
      "metadata": {
        "id": "EkaQcKhIW68m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables for input - note that these will depend upon a given itinerary. Not all books will have margin annotations.\n",
        "min_maintext_vpos = 100 #The minimum coordinate for main text\n",
        "max_maintext_vpos = 2250 #The maximum coordinate for main text\n",
        "#margin_indicator = \"—\""
      ],
      "metadata": {
        "id": "vf0M5nY5a4sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting data"
      ],
      "metadata": {
        "id": "FeIivfrIW5X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page'\n",
        "output_filename = 'page.csv'"
      ],
      "metadata": {
        "id": "J-suFdueW_bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract data from multiple XML files and save as a CSV and DataFrame\n",
        "def extract_text_lines_from_directory(directory, output_csv_file):\n",
        "    # Define the namespace\n",
        "    ns = {'ns': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
        "\n",
        "    # List to store all extracted data\n",
        "    all_data = []\n",
        "\n",
        "    # Walk through the directory and its subdirectories\n",
        "    for subdir, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            fpath = os.path.join(subdir, filename)\n",
        "\n",
        "            # Process only XML files\n",
        "            if fpath.endswith(\".xml\"):\n",
        "                try:\n",
        "                    tree = ET.parse(fpath)\n",
        "                    root = tree.getroot()\n",
        "\n",
        "                    # Safely extract relevant metadata from TranskribusMetadata\n",
        "                    transkribus_metadata = root.find(\".//ns:TranskribusMetadata\", ns)\n",
        "                    docId = transkribus_metadata.get('docId') if transkribus_metadata is not None else None\n",
        "                    pageId = transkribus_metadata.get('pageId') if transkribus_metadata is not None else None\n",
        "                    pageNr = transkribus_metadata.get('pageNr') if transkribus_metadata is not None else None\n",
        "\n",
        "                    # Iterate through each TextRegion element\n",
        "                    for textregion in root.findall(\".//ns:TextRegion\", ns):\n",
        "                        region_id = textregion.get(\"id\")\n",
        "                        region_custom = textregion.get(\"custom\")\n",
        "\n",
        "                        # Iterate through each TextLine element within the TextRegion\n",
        "                        for textline in textregion.findall(\".//ns:TextLine\", ns):\n",
        "                            line_id = textline.get(\"id\")\n",
        "                            custom = textline.get(\"custom\")\n",
        "                            coords = textline.find(\".//ns:Coords\", ns)\n",
        "                            points = coords.get(\"points\") if coords is not None else None\n",
        "                            unicode_elem = textline.find(\".//ns:Unicode\", ns)\n",
        "                            content = unicode_elem.text if unicode_elem is not None else None\n",
        "\n",
        "                            # Store the extracted data in a list\n",
        "                            all_data.append([docId, pageId, pageNr, region_id, region_custom, line_id, custom, points, content])\n",
        "\n",
        "                except ET.ParseError as e:\n",
        "                    print(f\"Error parsing {fpath}: {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {fpath}: {e}\")\n",
        "\n",
        "    # Convert the collected data into a DataFrame\n",
        "    columns = [\"docId\", \"pageId\", \"pageNr\", \"region_id\", \"region_custom\", \"line_id\", \"custom\", \"points\", \"content\"]\n",
        "    if all_data:\n",
        "        extract_df = pd.DataFrame(all_data, columns=columns)\n",
        "    else:\n",
        "        # Create an empty DataFrame if no data was collected\n",
        "        extract_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "    # Return the DataFrame\n",
        "    return extract_df\n",
        "\n",
        "# Example usage\n",
        "directory = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page'\n",
        "output_filename = 'page.csv'\n",
        "\n",
        "extract_df = extract_text_lines_from_directory(directory, output_filename)\n",
        "\n",
        "# Check the result\n",
        "print(extract_df)\n"
      ],
      "metadata": {
        "id": "V0Xxa7-RITmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/output.csv', index=False)\n",
        "extract_df"
      ],
      "metadata": {
        "id": "joyKgeA4bs9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_lines_from_directory(directory, output_csv_file):\n",
        "    # Define the namespace\n",
        "    ns = {'ns': 'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15'}\n",
        "\n",
        "    # List to store all extracted data\n",
        "    all_data = []\n",
        "\n",
        "    # Walk through the directory and its subdirectories\n",
        "    for subdir, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            fpath = os.path.join(subdir, filename)\n",
        "\n",
        "            # Process only XML files\n",
        "            if fpath.endswith(\".xml\"):\n",
        "                try:\n",
        "                    tree = ET.parse(fpath)\n",
        "                    root = tree.getroot()\n",
        "\n",
        "                    # Safely extract relevant metadata from TranskribusMetadata\n",
        "                    transkribus_metadata = root.find(\".//ns:TranskribusMetadata\", ns)\n",
        "                    docId = transkribus_metadata.get('docId') if transkribus_metadata is not None else None\n",
        "                    pageId = transkribus_metadata.get('pageId') if transkribus_metadata is not None else None\n",
        "                    pageNr = transkribus_metadata.get('pageNr') if transkribus_metadata is not None else None\n",
        "\n",
        "                    # Iterate through each TextRegion element\n",
        "                    for textregion in root.findall(\".//ns:TextRegion\", ns):\n",
        "                        region_id = textregion.get(\"id\")\n",
        "                        region_custom = textregion.get(\"custom\")\n",
        "\n",
        "                        # Iterate through each TextLine element within the TextRegion\n",
        "                        for textline in textregion.findall(\".//ns:TextLine\", ns):\n",
        "                            line_id = textline.get(\"id\")\n",
        "                            custom = textline.get(\"custom\")\n",
        "                            coords = textline.find(\".//ns:Coords\", ns)\n",
        "                            points = coords.get(\"points\") if coords is not None else None\n",
        "                            unicode_elem = textline.find(\".//ns:Unicode\", ns)\n",
        "                            content = unicode_elem.text if unicode_elem is not None else None\n",
        "\n",
        "                            # Store the extracted data in a list\n",
        "                            all_data.append([docId, pageId, pageNr, region_id, region_custom, line_id, custom, points, content])\n",
        "\n",
        "                except ET.ParseError as e:\n",
        "                    print(f\"Error parsing {fpath}: {e}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {fpath}: {e}\")\n",
        "\n",
        "    # Convert the collected data into a DataFrame\n",
        "    if all_data:\n",
        "        columns = [\"docId\", \"pageId\", \"pageNr\", \"region_id\", \"region_custom\", \"line_id\", \"custom\", \"points\", \"content\"]\n",
        "        extract_df = pd.DataFrame(all_data, columns=columns)\n",
        "        return extract_df\n",
        "    else:\n",
        "        # Return an empty DataFrame with the same columns if no data was found\n",
        "        columns = [\"docId\", \"pageId\", \"pageNr\", \"region_id\", \"region_custom\", \"line_id\", \"custom\", \"points\", \"content\"]\n",
        "        return pd.DataFrame(columns=columns)\n",
        "\n",
        "# Example usage\n",
        "directory = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page'\n",
        "output_filename = 'page.csv'\n",
        "extract_df = extract_text_lines_from_directory(directory, output_filename)"
      ],
      "metadata": {
        "id": "gfCaNryTdPNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/output.csv', index=False)\n",
        "extract_df"
      ],
      "metadata": {
        "id": "_sRxrwjJdY1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Tidying (tidy_df)\n",
        "This section takes the extract_df (output.csv) and adds columns helpful for sorting, later categorical analysis, and further tidying"
      ],
      "metadata": {
        "id": "vTPeNwf2giYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional: Reimport and view result of Text to Data\n",
        "#extract_df = pd.read_csv(\"/content/drive/Shareddrives/EmDigIt/Processing/extract_df.csv\")\n",
        "tidy_df = extract_df\n",
        "\n",
        "#Trim whitespace\n",
        "tidy_df['content'] = tidy_df['content'].str.replace(r'\\s+', ' ').str.strip()\n",
        "tidy_df"
      ],
      "metadata": {
        "id": "UZV1wQDLgjX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing code"
      ],
      "metadata": {
        "id": "udhBALjqjNAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process each row in the points column\n",
        "def extract_positions(points):\n",
        "    # Split the points string into a list of pairs (e.g., \"423,260\")\n",
        "    pairs = points.split()\n",
        "\n",
        "    # Split each pair into horizontal and vertical components and convert them to integers\n",
        "    hpos = [int(pair.split(',')[0]) for pair in pairs]  # Extract horizontal positions\n",
        "    vpos = [int(pair.split(',')[1]) for pair in pairs]  # Extract vertical positions\n",
        "\n",
        "    # Calculate max and min for both horizontal and vertical positions\n",
        "    max_hpos = max(hpos)\n",
        "    min_hpos = min(hpos)\n",
        "    max_vpos = max(vpos)\n",
        "    min_vpos = min(vpos)\n",
        "\n",
        "    return pd.Series([max_hpos, min_hpos, max_vpos, min_vpos])\n",
        "\n",
        "# Apply the function to each row in the \"points\" column\n",
        "tidy_df[['max_hpos', 'min_hpos', 'max_vpos', 'min_vpos']] = tidy_df['points'].apply(extract_positions)\n",
        "\n",
        "# Check the updated DataFrame\n",
        "tidy_df"
      ],
      "metadata": {
        "id": "mnBYd16CjNUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for additional Columns - same as before\n"
      ],
      "metadata": {
        "id": "-vzAdBdu9uAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create page, region, and line columns\n",
        "tidy_df[\"page\"] = tidy_df[\"pageNr\"].astype('int64')\n",
        "tidy_df['region'] = tidy_df['region_custom'].str.extract(r'readingOrder\\s*{index:(\\d+);}', expand=False).astype(int) + 1\n",
        "tidy_df[\"line\"] = tidy_df['custom'].str.extract(r'readingOrder\\s*{index:(\\d+);}', expand=False).astype(int) + 1\n",
        "\n",
        "# Sort by page and region\n",
        "tidy_df = tidy_df.sort_values(['page', 'region'])\n",
        "\n",
        "# Create page_region_line column\n",
        "tidy_df['page_region'] = 'P' + tidy_df['page'].astype(str) + 'R' + tidy_df['region'].astype(str)\n",
        "tidy_df['page_region_line'] = 'P' + tidy_df['page'].astype(str) + 'R' + tidy_df['region'].astype(str) + 'L' + tidy_df['line'].astype(str)\n",
        "\n",
        "# Select columns\n",
        "columns = [\"docId\", \"page\", \"region\", \"line\", \"page_region\", \"page_region_line\", \"content\", \"min_vpos\", \"max_vpos\", \"min_hpos\", \"max_hpos\"]\n",
        "tidy_df = pd.DataFrame(tidy_df, columns=columns)\n",
        "\n",
        "tidy_df\n"
      ],
      "metadata": {
        "id": "tRDROlqEkOJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Normalize spaces\n",
        "tidy_df['content'] = tidy_df['content'].str.replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "# Function to handle p. combinations\n",
        "def standardize_p_abbr(match):\n",
        "    following_char = match.group(1).lower()\n",
        "    return ' P.1' if following_char in ['l', 'i'] else f' P.{following_char}'\n",
        "\n",
        "# Dictionary of abbreviations and normalizations\n",
        "abbrevs = {\n",
        "    'ā': 'an', 'ō': 'on', 'õ': 'on', 'ē': 'en', 'ꝑ': 'per',\n",
        "    'Gio\\\\.': \"Giovanni\", 'Giac\\\\.': 'Giacomo',\n",
        "    r'\\b[Cc]it tà\\b': 'città', r'\\b[Cc]a stello\\b': 'castello',\n",
        "    r'\\b[Pp]ro vincia\\b': 'provincia', r'\\b[Pp]rinci pato\\b': 'principato',\n",
        "    'â': 'à', '-': '¬', '=': '¬', '&': 'e',\n",
        "    r'\\.\\.': '.', r' \\.': '.',\n",
        "}\n",
        "\n",
        "# Apply abbreviations\n",
        "for old, new in abbrevs.items():\n",
        "    tidy_df['content'] = tidy_df['content'].str.replace(old, new, regex=True)\n",
        "\n",
        "# Apply p. abbreviation normalization\n",
        "tidy_df['content'] = tidy_df['content'].str.replace(r'(?i)\\s+p\\. ?([lLi0-9])', standardize_p_abbr, regex=True)\n",
        "\n",
        "# Decimal symbols and their replacements\n",
        "decimals = {\n",
        "    r'(?<=\\d)[\\.,]?\\*': \".5\",\n",
        "}\n",
        "for old, new in decimals.items():\n",
        "    tidy_df['content'] = tidy_df['content'].str.replace(old, new, regex=True)\n",
        "\n",
        "# Create a DataFrame for short entries at the bottom of the page\n",
        "cw_df = tidy_df.loc[(tidy_df['content'].str.len() <= 6) & (tidy_df['max_vpos'] >= max_maintext_vpos)]\n",
        "\n",
        "# Drop these entries from the main DataFrame\n",
        "tidy_df = tidy_df.drop(cw_df.index)\n",
        "\n",
        "# Resulting tidy_df contains cleaned content\n",
        "tidy_df\n"
      ],
      "metadata": {
        "id": "SVCS5kAb8Go6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving everything to a CSV file"
      ],
      "metadata": {
        "id": "X5R46kBSHvpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the correct directory and output filename\n",
        "directory = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page'\n",
        "output_filename = 'tidy_df.csv'\n",
        "\n",
        "# Full file path\n",
        "file_path = f\"{directory}/{output_filename}\"\n",
        "\n",
        "# Save the DataFrame to the specified location\n",
        "tidy_df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "3RVmjVrnHyMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tidy_df.head())\n"
      ],
      "metadata": {
        "id": "0PTYvfzhISgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The optimized code processes text data from a DataFrame by aggregating rows based on page_region, cleaning and standardizing text (removing unnecessary separators and hyphens), and splitting concatenated text into individual rows. It adds line numbers and unique identifiers (page_region_line) for each line, organizes the data by page and region, and outputs two structured CSV files: one for aggregated text regions and another for individual text lines."
      ],
      "metadata": {
        "id": "h9xRvnFyKLiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load the tidy_df DataFrame\n",
        "tidy_df = pd.read_csv(\"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/tidy_df.csv\")\n",
        "\n",
        "# Make a copy of the DataFrame\n",
        "tr_df = tidy_df.drop(columns=['page_region_line']).copy()\n",
        "\n",
        "# Add pipe symbol to the end of each content\n",
        "tr_df['content'] = tr_df['content'].astype(str) + '|'\n",
        "\n",
        "# Aggregate data by page_region\n",
        "agg_functions = {\n",
        "    'page': 'first',\n",
        "    'region': 'first',\n",
        "    'content': ' '.join,\n",
        "    'max_vpos': 'max',\n",
        "    'max_hpos': 'max',\n",
        "    'min_vpos': 'min',\n",
        "    'min_hpos': 'min',\n",
        "    'docId': 'first'\n",
        "}\n",
        "tr_df = tr_df.groupby('page_region', as_index=False).agg(agg_functions)\n",
        "\n",
        "# Save the text regions DataFrame to CSV\n",
        "tr_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/text_regions.csv', index=False)\n",
        "\n",
        "# Function to remove separators\n",
        "def remove_separator(text):\n",
        "    pattern = r\"(?<![0-9]\\.)(?<![0-9])(?<!\\.)(?<!\\—)\\|(?!Da|DA|[AÀaà] |[Aa]l |Parte\\b|Poste\\b|Leghe\\b|Viaggio\\b|Qui\\b|Parti\\b)\"\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# Apply separator removal and replace hyphens\n",
        "tr_df['content'] = tr_df['content'].apply(remove_separator).str.replace('¬', '-', regex=False).str.replace('-', '', regex=False)\n",
        "\n",
        "# Split content based on the pipe separator and explode into new rows\n",
        "nl_df = tr_df.copy()\n",
        "nl_df[\"content\"] = nl_df[\"content\"].str.split(\"|\")\n",
        "nl_df = nl_df.explode('content')\n",
        "\n",
        "# Trim whitespace and filter non-empty rows\n",
        "nl_df['content'] = nl_df['content'].str.strip()\n",
        "nl_df = nl_df[nl_df['content'].str.len() > 0]\n",
        "\n",
        "# Sort by page and region\n",
        "nl_df = nl_df.sort_values(['page', 'region']).reset_index(drop=True)\n",
        "\n",
        "# Add line and page_region_line columns\n",
        "nl_df['line'] = nl_df.groupby('page_region').cumcount() + 1\n",
        "nl_df['page_region_line'] = \"P\" + nl_df[\"page\"].astype(str) + \"R\" + nl_df[\"region\"].astype(str) + \"L\" + nl_df['line'].astype(str)\n",
        "\n",
        "# Export the new line DataFrame to CSV\n",
        "nl_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/new_lines.csv', index=False)\n",
        "\n",
        "nl_df\n"
      ],
      "metadata": {
        "id": "HOTni79OnGkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Followed by automated Tagging"
      ],
      "metadata": {
        "id": "HG2u5UzmdMUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tg_df = nl_df"
      ],
      "metadata": {
        "id": "_AF9NxDjdLqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code systematically classifies text lines into predefined categories (page-number, page-header, margin-annotation, chapter-heading, route-header, sum-distance, prose, or location) based on their content, length, and position, ensuring accurate tagging for further analysis."
      ],
      "metadata": {
        "id": "yygh73M-cEb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the margin indicator\n",
        "margin_indicator = \"-\"  # Replace with the correct margin symbol in your dataset\n",
        "\n",
        "# Identify page-numbers and page-headers based on their position and content\n",
        "tg_df.loc[tg_df['content'].str.match(r'^\\d+$', na=False) & (tg_df['min_vpos'] <= min_maintext_vpos), 'line_type'] = 'page-number'\n",
        "tg_df.loc[~tg_df['content'].str.match(r'^\\d+$', na=False) & (tg_df['min_vpos'] <= min_maintext_vpos), 'line_type'] = 'page-header'\n",
        "\n",
        "# Identify page numbers globally if not restricted to the top of the page\n",
        "tg_df.loc[tg_df['content'].str.match(r'^\\d+$', na=False), 'line_type'] = 'page-number'\n",
        "\n",
        "# Tag marginal annotations or glosses if margin symbols are present\n",
        "tg_df.loc[tg_df['content'].str.contains(margin_indicator, na=False), 'line_type'] = 'margin-annotation'\n",
        "regions_with_margin_annotation = tg_df[tg_df['line_type'] == 'margin-annotation']['page_region'].unique()\n",
        "tg_df.loc[tg_df['page_region'].isin(regions_with_margin_annotation), 'line_type'] = 'margin-annotation'\n",
        "\n",
        "# Identify chapter headings based on keywords\n",
        "chapter_patterns = [\n",
        "    r'^Capitolo', r'^CAPITOLO', r'^ITINERARIO', r'^Parti ', r'^Libro', r'^LE POSTE',\n",
        "    r'^POSTE', r'^D\\'ITALIA', r'^Poste per diverse'\n",
        "]\n",
        "for pattern in chapter_patterns:\n",
        "    tg_df.loc[tg_df['content'].str.contains(pattern, regex=True, na=False), 'line_type'] = 'chapter-heading'\n",
        "\n",
        "# Identify route headers based on keywords\n",
        "route_patterns = [\n",
        "    r'^Poste d', r'^Viaggio ', r'^Altra ', r'^Communicazione ', r'^Strada '\n",
        "]\n",
        "for pattern in route_patterns:\n",
        "    tg_df.loc[tg_df['content'].str.contains(pattern, regex=True, na=False), 'line_type'] = 'route-header'\n",
        "\n",
        "# Identify sum-distances based on unit patterns\n",
        "sum_ind = [r'^Nu\\.', r'^nu\\.', r'^[Pp]oste \\d', r'^[Ll]eghe \\d', r'^[Mm]iglia \\d']\n",
        "for pattern in sum_ind:\n",
        "    tg_df.loc[tg_df['content'].str.contains(pattern, regex=True, na=False), 'line_type'] = 'sum-distance'\n",
        "\n",
        "# Tag long texts as \"prose\"\n",
        "tg_df.loc[tg_df['content'].str.len() > 180, 'line_type'] = 'prose'\n",
        "\n",
        "# Set remaining undefined values to \"location\"\n",
        "tg_df['line_type'] = tg_df['line_type'].astype(str).replace('nan', np.nan)\n",
        "tg_df['line_type'] = tg_df['line_type'].fillna('location')\n",
        "\n",
        "# Optional: Access rows tagged as \"prose\" for review\n",
        "tg_df.loc[tg_df['line_type'] == 'prose'].sample(20)\n",
        "\n"
      ],
      "metadata": {
        "id": "1Avrb1mnoWBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correcting Multiple Pages (concat_df)\n"
      ],
      "metadata": {
        "id": "W5kMA6aqABex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set fix_df to tg_df\n",
        "fix_df = tg_df\n",
        "\n",
        "# Filter out rows with fix_df['line_type'] in specified categories\n",
        "filtered_df = fix_df[~fix_df['line_type'].isin(['page-number', 'chapter-heading', 'page-header'])]\n",
        "\n",
        "# Initialize a list to store the modified rows\n",
        "modified_rows = []\n",
        "\n",
        "# Initialize a flag to track whether to skip rows\n",
        "skip = False\n",
        "\n",
        "# Iterate through unique pages\n",
        "for page in filtered_df['page'].unique():\n",
        "    # Get rows for the current page\n",
        "    page_rows = filtered_df[filtered_df['page'] == page]\n",
        "    max_region_row = page_rows.iloc[-1]\n",
        "\n",
        "    # Get rows for the next page, if any\n",
        "    next_page_rows = filtered_df[filtered_df['page'] == page + 1]\n",
        "    min_region_row = next_page_rows.iloc[0] if not next_page_rows.empty else None\n",
        "\n",
        "    if (\n",
        "        min_region_row is not None and\n",
        "        max_region_row['line_type'] in ['location', 'prose'] and\n",
        "        min_region_row['line_type'] in ['location', 'prose'] and\n",
        "        not max_region_row['content'].strip().endswith(tuple(string.digits))\n",
        "    ):\n",
        "        # Combine content if conditions are met\n",
        "        combined_content = f\"{max_region_row['content']} {min_region_row['content']}\".strip()\n",
        "        combined_row = max_region_row.copy()\n",
        "        combined_row['content'] = combined_content\n",
        "\n",
        "        # Add rows to the modified list\n",
        "        modified_rows.extend(page_rows.iloc[:-1].values.tolist() if not skip else page_rows.iloc[1:-1].values.tolist())\n",
        "        modified_rows.append(combined_row.values.tolist())\n",
        "\n",
        "        # Set skip for the next iteration\n",
        "        skip = True\n",
        "    else:\n",
        "        # Add rows to the modified list without combination\n",
        "        modified_rows.extend(page_rows.iloc[1:].values.tolist() if skip else page_rows.values.tolist())\n",
        "        skip = False\n",
        "\n",
        "# Create a new DataFrame with modified rows\n",
        "modified_df = pd.DataFrame(modified_rows, columns=fix_df.columns)\n",
        "\n",
        "modified_df #to print the results"
      ],
      "metadata": {
        "id": "hP_2v6OL0FtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-add previously filtered data\n",
        "filtered_df_to_add_back = fix_df[fix_df['line_type'].isin(['page-number', 'chapter-heading', 'page-header'])]\n",
        "concat_df = pd.concat([filtered_df_to_add_back, modified_df], ignore_index=True)\n",
        "\n",
        "# Sort by page and region\n",
        "concat_df = concat_df.sort_values(['page', 'region']).reset_index(drop=True)\n",
        "\n",
        "# Renumber lines within each page_region\n",
        "concat_df['line'] = concat_df.groupby('page_region').cumcount() + 1\n",
        "\n",
        "# Add new page_region_line column\n",
        "concat_df['page_region_line'] = (\n",
        "    \"P\" + concat_df[\"page\"].astype(str) +\n",
        "    \"R\" + concat_df[\"region\"].astype(str) +\n",
        "    \"L\" + concat_df['line'].astype(str)\n",
        ")\n",
        "\n",
        "# Reorder the columns\n",
        "concat_df = concat_df[['docId', 'page_region_line', 'page_region', 'page', 'region', 'min_vpos', 'min_hpos',\n",
        "                       'max_vpos', 'max_hpos', 'line_type', 'content']]\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "output_path = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/new_lines.csv'\n",
        "concat_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"DataFrame successfully exported to {output_path}\")\n"
      ],
      "metadata": {
        "id": "il8-FAA3A8sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code to Automate Checks and Fix Common Errors --- check region assignments, validate distance/unit extraction, and correct common errors\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-HqI4lcsohCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the concat_lines.csv known as new Lines file\n",
        "concat_df = pd.read_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/new_lines.csv')\n",
        "\n",
        "# --- 1. Check Region Assignments ---\n",
        "# Verify that regions are sequential within each page\n",
        "def check_region_assignments(df):\n",
        "    invalid_regions = []\n",
        "    for page in df['page'].unique():\n",
        "        regions = df[df['page'] == page]['region'].tolist()\n",
        "        if sorted(regions) != regions:\n",
        "            invalid_regions.append(page)\n",
        "    return invalid_regions\n",
        "\n",
        "invalid_regions = check_region_assignments(concat_df)\n",
        "if invalid_regions:\n",
        "    print(f\"Invalid region assignments found in pages: {invalid_regions}\")\n",
        "else:\n",
        "    print(\"Region assignments look correct.\")\n",
        "\n",
        "# --- 2. Check Distance/Unit Extracts ---\n",
        "# Validate that distance and unit fields follow expected patterns\n",
        "def validate_distance_units(df, distance_pattern=r'\\d+(?:\\.\\d+)?', unit_pattern=r'(km|mi|m|yards)'):\n",
        "    invalid_rows = df[~df['content'].str.contains(f\"{distance_pattern} {unit_pattern}\", regex=True, na=False)]\n",
        "    return invalid_rows\n",
        "\n",
        "distance_issues = validate_distance_units(concat_df)\n",
        "if not distance_issues.empty:\n",
        "    print(f\"Distance/unit extraction issues found in {len(distance_issues)} rows:\")\n",
        "    print(distance_issues[['page', 'region', 'content']].head())\n",
        "else:\n",
        "    print(\"Distance/unit extraction looks correct.\")\n",
        "\n",
        "# --- 3. Check Line Content Parsing ---\n",
        "# Ensure line content meets specific formatting expectations\n",
        "def validate_line_content(df):\n",
        "    # Example: Ensure no rows have trailing/leading whitespace or inconsistent punctuation\n",
        "    df['content'] = df['content'].str.strip()\n",
        "    issues = df[df['content'].str.match(r'.*\\s+|[.,!?]{2,}', na=False)]\n",
        "    return issues\n",
        "\n",
        "content_issues = validate_line_content(concat_df)\n",
        "if not content_issues.empty:\n",
        "    print(f\"Line content parsing issues found in {len(content_issues)} rows:\")\n",
        "    print(content_issues[['page', 'region', 'content']].head())\n",
        "else:\n",
        "    print(\"Line content parsing looks correct.\")\n",
        "\n",
        "# --- 4. Replace Common Errors ---\n",
        "# Define a dictionary of common misspellings or errors and their corrections\n",
        "corrections = {\n",
        "    'recieve': 'receive',\n",
        "    'adress': 'address',\n",
        "    'teh': 'the',\n",
        "    'lable': 'label'\n",
        "}\n",
        "\n",
        "# Apply corrections\n",
        "for wrong, correct in corrections.items():\n",
        "    concat_df['content'] = concat_df['content'].str.replace(rf'\\b{wrong}\\b', correct, regex=True)\n",
        "\n",
        "print(\"Common errors have been replaced.\")\n",
        "\n",
        "# --- Export the corrected DataFrame ---\n",
        "output_path = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/concat_lines_checked.csv'\n",
        "concat_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Checked and corrected data exported to {output_path}\")"
      ],
      "metadata": {
        "id": "ZJ65GbR1o8Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Location Matching - gloc_df and loc_df\n"
      ],
      "metadata": {
        "id": "p44-Rz9QrtsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup and From preceeding and Following text"
      ],
      "metadata": {
        "id": "tc1Eh-Khttxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "file_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/concat_lines_checked.csv\"\n",
        "\n",
        "# Check if the file exists and load it\n",
        "if os.path.exists(file_path):\n",
        "    concat_df = pd.read_csv(file_path)\n",
        "    print(f\"File '{file_path}' loaded successfully!\")\n",
        "else:\n",
        "    print(f\"Error: File not found at '{file_path}'.\")\n",
        "    print(\"Available files in the directory:\")\n",
        "    directory = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page\"\n",
        "    if os.path.exists(directory):\n",
        "        print(os.listdir(directory))\n",
        "    else:\n",
        "        print(f\"Directory not found: {directory}\")\n",
        "    raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "# --- 2. Filter Rows for \"location\" ---\n",
        "loc_df = concat_df[concat_df['line_type'] == 'location'].copy()\n",
        "\n",
        "# Clean the content column\n",
        "loc_df['cleaned'] = (\n",
        "    loc_df['content']\n",
        "    .fillna('')  # Handle NaN values\n",
        "    .str.lower()  # Convert to lowercase\n",
        "    .str.replace(r'\\s([lL]|[Pp])\\.\\d+|\\s\\.\\d+$', '', regex=True)  # Remove trailing page/line numbers\n",
        "    .str.replace(r'^\\d+', '', regex=True)  # Remove digits at the beginning\n",
        "    .str.strip()  # Trim whitespace\n",
        ")\n",
        "\n",
        "# Remove rows with empty content after cleaning\n",
        "loc_df = loc_df[loc_df['cleaned'].str.len() > 0]\n",
        "\n",
        "# --- 3. Extract Text After Prepositions ---\n",
        "preps = ['A', 'Alla', 'Al', 'Alle', 'a', 'à', 'alle', 'al', 'a la', 'ala', 'alas',\n",
        "         'all\\'', 'alla', 'allas', 'ale', 'ad', 'allo', 'auff', 'da', 'zu', 'zur', 'zum']\n",
        "\n",
        "def extract_text_after_preps(text, prepositions):\n",
        "    \"\"\"Extract text after a preposition.\"\"\"\n",
        "    for prep in prepositions:\n",
        "        match = re.search(rf'\\b{prep}\\b\\s+(.+)', text)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    return text\n",
        "\n",
        "loc_df['cleaned'] = loc_df['cleaned'].apply(lambda x: extract_text_after_preps(x, preps))\n",
        "\n",
        "# --- 4. Keep Text Before First Punctuation ---\n",
        "def keep_before_punctuation(text):\n",
        "    \"\"\"Keep only the text before the first punctuation.\"\"\"\n",
        "    match = re.match(r'^([^\\.!?;,]*)', text)\n",
        "    return match.group(1).strip() if match else text\n",
        "\n",
        "loc_df['cleaned'] = loc_df['cleaned'].apply(keep_before_punctuation)\n",
        "\n",
        "# --- 5. Export Cleaned Data ---\n",
        "output_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/output2.csv\"\n",
        "loc_df.to_csv(output_path, index=False)\n",
        "print(f\"File saved to: {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "u2BkVfbFtSda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions and 0ther Non-location data"
      ],
      "metadata": {
        "id": "308AJ2TY0CIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Following Text: Keeps only the text that precedes the details provided in the dictionary below\n",
        "\n",
        "details = [\n",
        "    'per', 'e per', 'per', 'e', 'ò', 'o', '\\&', 'p', 'miglia', 'm', 'mi', 'leg', 'l', 'le', 'leghe', 'leghi', '\\d+',\n",
        "    'città', 'citta', 'metropoli', 'villa', 'uilla', 'villagio', 'villaggio', 'uillagio', 'borgo', 'castello',\n",
        "    'castellano', 'hosteria', 'osteria', 'capo', 'corte', 'principato', 'provincia', 'chiesa', 'fiume', 'stato',\n",
        "    'montagna', 'grosso', 'bellissima', 'già', 'ove', \"questo\", 'havete', 'che', 'vicina', 'vicino', 'dietro',\n",
        "    'dietra', 'come'\n",
        "]\n",
        "\n",
        "for i in loc_df.index:\n",
        "    for detail in details:\n",
        "        if re.search(fr\"\\b{detail}\\b|\\d\", loc_df.loc[i, 'cleaned']):\n",
        "            match = re.search(fr\"\\b{re.escape(detail)}\\b\", loc_df.loc[i, 'cleaned'])\n",
        "            if match:\n",
        "                split_point = match.start()\n",
        "                loc_df.loc[i, 'cleaned'] = loc_df.loc[i, 'cleaned'][:split_point].strip()\n",
        "                break\n",
        "\n",
        "# Preceding Text: Keeps only the text that follows the instructions provided in the dictionary below\n",
        "\n",
        "instr = [\n",
        "    \"passate\", \"passarete\", \"si ascende\", \"ascenderete\", \"si imbarca\", \"se imbarca\", \"volete\", \"volendo\", \"vedrete\",\n",
        "    \"trovate\", \"cominciate\", \"si passa\", \"passare\", \"qui\", \"qua\", \"quà\", \"quella\", \"quello\", \"quelle\", \"quelli\",\n",
        "    \"questo\", \"questa\", \"questi\", \"queste\", \"poi\", \"volendo\", 'gran', \"guardatevi\", \"guardate\", \"imbarcatevi\",\n",
        "    \"ove\", \"pigliate\", \"da detta\", \"da questa\", \"poste\", \"viaggio\", \"quì\", \"sono\", \"una\"\n",
        "]\n",
        "\n",
        "for i in loc_df.index:\n",
        "    for instruction in instr:\n",
        "        if instruction in loc_df.loc[i, 'cleaned']:\n",
        "            match = re.search(rf'\\b{instruction}\\b.*', loc_df.loc[i, 'cleaned'])\n",
        "            if match:\n",
        "                loc_df.loc[i, 'cleaned'] = ''\n",
        "                break\n",
        "\n",
        "# Remove short entries: Set to blank if consisting of two characters or fewer\n",
        "\n",
        "\n",
        "for i in loc_df.index:\n",
        "    if len(loc_df.loc[i, 'cleaned']) <= 2:\n",
        "        loc_df.loc[i, 'cleaned'] = ''\n",
        "\n",
        "# Save the final cleaned DataFrame to a CSV file\n",
        "\n",
        "loc_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/locations_extract.csv', index=False)"
      ],
      "metadata": {
        "id": "xQH43C4e2Atj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code to avoid hand checking locations extract **\n"
      ],
      "metadata": {
        "id": "zCE67JJU4VFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the locations_extract.csv file\n",
        "file_path = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/locations_extract.csv'\n",
        "loc_df = pd.read_csv(file_path)\n",
        "\n",
        "# Define a function to check and flag potential issues\n",
        "def check_and_flag_locations(row):\n",
        "    # Flag word fragments or entries with special characters\n",
        "    if re.search(r'\\b\\w{1,2}\\b', row) or re.search(r'[^a-zA-Z\\s]', row):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Apply the flagging function to the 'cleaned' column\n",
        "loc_df['flag'] = loc_df['cleaned'].apply(lambda x: check_and_flag_locations(x) if isinstance(x, str) else False)\n",
        "\n",
        "# Filter rows that are flagged for manual inspection\n",
        "flagged_df = loc_df[loc_df['flag'] == True]\n",
        "\n",
        "# Function to clean common errors\n",
        "def correct_common_errors(text):\n",
        "    corrections = {\n",
        "        'uillagio': 'villaggio',\n",
        "        'citta': 'città',\n",
        "        'metropoli ': 'metropoli',\n",
        "        # Add more common corrections as needed\n",
        "    }\n",
        "    for error, correction in corrections.items():\n",
        "        text = re.sub(rf'\\b{error}\\b', correction, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "# Apply the correction function to the 'cleaned' column\n",
        "loc_df['cleaned'] = loc_df['cleaned'].apply(lambda x: correct_common_errors(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Save flagged rows for manual review\n",
        "flagged_file_path = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/flagged_locations.csv'\n",
        "flagged_df.to_csv(flagged_file_path, index=False)\n",
        "\n",
        "# Save the corrected DataFrame back to CSV\n",
        "corrected_file_path = '/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/corrected_locations_extract.csv'\n",
        "loc_df.to_csv(corrected_file_path, index=False)\n",
        "\n",
        "print(\"Flagged rows saved to:\", flagged_file_path)\n",
        "print(\"Corrected locations file saved to:\", corrected_file_path)\n"
      ],
      "metadata": {
        "id": "X7GO6DN64dO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matching to Gazzeteer ---- missing Gazzater Files\n",
        "\n"
      ],
      "metadata": {
        "id": "mEkj3cNi7gs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Load the gazetteer CSV from the correct directory\n",
        "gaz_df = pd.read_csv(\"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/gazetteer.csv\")\n",
        "\n",
        "# Reimport and refilter data if gazetteer changes\n",
        "reimport_df = pd.read_csv(\"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/full_test_df.csv\")\n",
        "locations_filter = reimport_df['line_type'] == 'location'\n",
        "loc_df = reimport_df[locations_filter].copy()\n",
        "\n",
        "# Display the DataFrame\n",
        "print(loc_df)\n",
        "\n",
        "# Split the Location_Name field based on the pipe separator\n",
        "gaz_df[\"Location_Name\"] = gaz_df[\"Location_Name\"].str.split(\"|\")\n",
        "gaz_df = gaz_df.explode('Location_Name')\n",
        "\n",
        "# Create the new geolocated dataframe based on loc_df\n",
        "gloc_df = loc_df.copy()\n",
        "\n",
        "# Create a dictionary to map routes to their associated states\n",
        "route_state_dict = gloc_df.groupby('route_description')['state'].apply(lambda x: x.unique()).to_dict()\n",
        "\n",
        "# Function to find fuzzy matches with a threshold for accuracy\n",
        "def find_fuzzy_match_with_state(row, route_state_dict, threshold=70):\n",
        "    if pd.isna(row['cleaned']):\n",
        "        return None\n",
        "\n",
        "    # Get matching states for the current route\n",
        "    matching_states = route_state_dict.get(row['route_description'], [])\n",
        "\n",
        "    # If no matching states, return None\n",
        "    if len(matching_states) == 0:\n",
        "        return None\n",
        "\n",
        "    # Filter gazetteer entries based on matching states\n",
        "    filtered_gaz_df = gaz_df[gaz_df['state'].isin(matching_states)]\n",
        "\n",
        "    # Check for an exact match\n",
        "    exact_match = filtered_gaz_df[filtered_gaz_df['Location_Name'] == row['cleaned']]\n",
        "    if not exact_match.empty:\n",
        "        return exact_match['id'].values[0]\n",
        "\n",
        "    # Fuzzy matching\n",
        "    max_ratio = -1\n",
        "    best_match = None\n",
        "    for index, gaz_row in filtered_gaz_df.iterrows():\n",
        "        ratio = fuzz.token_set_ratio(row['cleaned'], gaz_row['Location_Name'])\n",
        "        if ratio > threshold and ratio > max_ratio:\n",
        "            max_ratio = ratio\n",
        "            best_match = gaz_row['id']\n",
        "\n",
        "    return best_match\n",
        "\n",
        "# Function to handle user input and apply fuzzy matching\n",
        "def process_fuzzy_matching(gloc_df, route_state_dict):\n",
        "    user_input = input(\"Enter 'all' to run on all rows or 'missing' to run on rows with missing match_id: \").strip().lower()\n",
        "\n",
        "    if user_input == 'all':\n",
        "        gloc_df['id'] = gloc_df.apply(lambda row: find_fuzzy_match_with_state(row, route_state_dict), axis=1)\n",
        "    elif user_input == 'missing':\n",
        "        missing_rows = gloc_df['id'].isnull()\n",
        "        gloc_df.loc[missing_rows, 'id'] = gloc_df[missing_rows].apply(lambda row: find_fuzzy_match_with_state(row, route_state_dict), axis=1)\n",
        "    else:\n",
        "        print(\"Invalid input. Please enter 'all' or 'missing'.\")\n",
        "\n",
        "# Apply fuzzy matching\n",
        "def main():\n",
        "    process_fuzzy_matching(gloc_df, route_state_dict)\n",
        "\n",
        "    # Error checkers: Ensure no duplicates or empty IDs\n",
        "    if gloc_df['id'].isnull().any():\n",
        "        print(\"Warning: Some rows still have missing IDs. Please review.\")\n",
        "    if gloc_df.duplicated(subset=['id']).any():\n",
        "        print(\"Warning: Duplicate IDs found. Consider resolving conflicts.\")\n",
        "\n",
        "    # Save the output to the correct directory\n",
        "    output_path = \"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/geolocated.csv\"\n",
        "    gloc_df.to_csv(output_path, index=False)\n",
        "    print(f\"Geolocated data saved to: {output_path}\")\n",
        "\n",
        "# Execute the script\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "1EDohSkEjDZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Levenshtein Match - Final option and All methods"
      ],
      "metadata": {
        "id": "TQlZ3oqJm8Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import Levenshtein\n",
        "\n",
        "# Set the threshold for fuzzy matching\n",
        "threshold = 0.8  # Adjust this value to your desired threshold (between 0 and 1)\n",
        "\n",
        "# Load the data\n",
        "gloc_df = pd.read_csv(\"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/geolocated.csv\")\n",
        "gaz_df = pd.read_csv(\"/path/to/gaz_df.csv\")  # Replace with the actual path to gaz_df\n",
        "\n",
        "# Drop unnecessary columns in gloc_df\n",
        "gloc_df = gloc_df.loc[:, :'id']\n",
        "\n",
        "# Reset indices\n",
        "gloc_df.reset_index(drop=True, inplace=True)\n",
        "gaz_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Function to find fuzzy match based on the threshold\n",
        "def find_fuzzy_match(row, gaz_df, threshold):\n",
        "    max_ratio = -1\n",
        "    best_match = None\n",
        "    for _, gaz_row in gaz_df.iterrows():\n",
        "        ratio = Levenshtein.ratio(row['cleaned'], gaz_row['Location_Name'])\n",
        "        if ratio > threshold and ratio > max_ratio:\n",
        "            max_ratio = ratio\n",
        "            best_match = gaz_row['new_id']\n",
        "    return best_match\n",
        "\n",
        "# Apply fuzzy matching to gloc_df\n",
        "gloc_df['match_id'] = gloc_df.apply(find_fuzzy_match, axis=1, args=(gaz_df, threshold))\n",
        "\n",
        "# Merge gloc_df with gaz_df based on the match_id and new_id columns\n",
        "merged_df = gloc_df.merge(\n",
        "    gaz_df[['geoname', 'id', 'geonameId', 'state', 'country_code', 'Location_Lat', 'Location_Lng']],\n",
        "    how='left', left_on='match_id', right_on='new_id'\n",
        ")\n",
        "\n",
        "# Drop duplicates\n",
        "merged_df = merged_df.drop_duplicates()\n",
        "\n",
        "# Export the merged dataframe to CSV\n",
        "merged_df.to_csv('/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/geolocated_merged.csv', index=False)"
      ],
      "metadata": {
        "id": "YIplWmowm71n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gazzetteer Adjustment to reflect new locations - both matched and unmatched"
      ],
      "metadata": {
        "id": "D911y18aosst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Re-import geolocated.csv, dropping unnecessary columns after 'match_id'\n",
        "gloc_df = pd.read_csv(\"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/geolocated_merged.csv\").loc[:, :'id']\n",
        "\n",
        "# Load gazetteer CSV\n",
        "new_gaz_df = pd.read_csv(\"/content/drive/Shareddrives/EmDigIt/Processing/gazetteer.csv\")\n",
        "\n",
        "# Efficiently map 'cleaned' values to 'Location_Name' where 'id' matches 'match_id'\n",
        "new_gaz_df.set_index('id', inplace=True)  # Set index for faster lookups\n",
        "gloc_df.set_index('id', inplace=True)\n",
        "\n",
        "# Update Location_Name by appending cleaned values where match_id is found\n",
        "new_gaz_df.loc[gloc_df.index.intersection(new_gaz_df.index), 'Location_Name'] += '|' + gloc_df['cleaned']\n",
        "\n",
        "# Reset index after modifications\n",
        "new_gaz_df.reset_index(inplace=True)\n",
        "gloc_df.reset_index(inplace=True)\n",
        "\n",
        "# Create a boolean mask for rows with missing match_id\n",
        "mask = gloc_df['match_id'].isna() | gloc_df['match_id'].isnull() | (gloc_df['match_id'] == '')\n",
        "\n",
        "# Filter gloc_df to extract new unmatched locations\n",
        "filtered_gloc_df = gloc_df.loc[mask, ['cleaned']].drop_duplicates().dropna().rename(columns={'cleaned': 'Location_Name'})\n",
        "filtered_gloc_df['Location_Name_Standardized'] = filtered_gloc_df['Location_Name']\n",
        "\n",
        "# Define structure for new entries with default empty values\n",
        "new_rows = pd.DataFrame({\n",
        "    'id': [''] * len(filtered_gloc_df),\n",
        "    'Location_Name': filtered_gloc_df['Location_Name'],\n",
        "    'Location_Name_Standardized': filtered_gloc_df['Location_Name_Standardized'],\n",
        "    'geonameId': [''] * len(filtered_gloc_df),\n",
        "    'adminName1': [''] * len(filtered_gloc_df),\n",
        "    'Location_Lat': [''] * len(filtered_gloc_df),\n",
        "    'Location_Lng': [''] * len(filtered_gloc_df),\n",
        "    'geoname': [''] * len(filtered_gloc_df),\n",
        "    'state': [''] * len(filtered_gloc_df),\n",
        "    'country_code': [''] * len(filtered_gloc_df),\n",
        "    'Flag': [''] * len(filtered_gloc_df)\n",
        "})\n",
        "\n",
        "# Append the new rows to new_gaz_df in one go (avoids slow row-by-row appends)\n",
        "new_gaz_df = pd.concat([new_gaz_df, new_rows], ignore_index=True)\n",
        "\n",
        "# Sort and reset index once at the end (instead of multiple times)\n",
        "new_gaz_df.sort_values('Location_Name_Standardized', inplace=True)\n",
        "new_gaz_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Renumber new_id with index values\n",
        "new_gaz_df['newest_id'] = new_gaz_df.index + 1\n",
        "\n",
        "# Corrected file path for exporting the new gazetteer\n",
        "new_gaz_df.to_csv(\"/content/drive/MyDrive/EmDigitPageFiles/GM1684/497635/GM1684/page/new_gaz.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "jofd7doepqCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Output\n"
      ],
      "metadata": {
        "id": "RhXl-A8ppzyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "# ----------------------------- STEP 1: IMPORT DATA -------------------------------- #\n",
        "\n",
        "# Import the CSV files into dataframes\n",
        "geolocated_df = pd.read_csv(\"/content/drive/Shareddrives/EmDigIt/Processing/geolocated.csv\")\n",
        "final_df = pd.read_csv(\"/content/drive/Shareddrives/EmDigIt/Processing/full_test_df.csv\")\n",
        "\n",
        "# -------------------------- STEP 2: MERGE GEOLOCATED DATA ------------------------- #\n",
        "\n",
        "# Merge final_df with geolocated_df using 'page_region_line', prioritizing new values\n",
        "merged_df = pd.merge(final_df, geolocated_df, on='page_region_line', how='left', suffixes=('_final', '_geolocated'))\n",
        "\n",
        "# Efficiently update values only where geolocated_df has non-null data\n",
        "for col in geolocated_df.columns:\n",
        "    if col != 'page_region_line':  # Avoid modifying the key column\n",
        "        merged_df[col] = merged_df[f'{col}_geolocated'].combine_first(merged_df[f'{col}_final'])\n",
        "\n",
        "# Drop redundant columns with suffixes\n",
        "merged_df = merged_df[final_df.columns]\n",
        "\n",
        "# Save the merged dataframe\n",
        "merged_df.to_csv('/content/drive/Shareddrives/EmDigIt/Processing/new_final.csv', index=False)\n",
        "\n",
        "# --------------------------- STEP 3: MERGE WITH GAZETTEER -------------------------- #\n",
        "\n",
        "# Import the gazetteer dataframe\n",
        "gaz_df = pd.read_csv(\"/content/drive/Shareddrives/EmDigIt/Processing/gazetteer.csv\")\n",
        "\n",
        "# Merge on 'id', keeping values from gaz_df where available\n",
        "final_merged_df = pd.merge(merged_df, gaz_df, on='id', how='left', suffixes=('', '_gaz'))\n",
        "\n",
        "# Efficiently update final_merged_df with gaz_df values where available\n",
        "for col in gaz_df.columns:\n",
        "    if col != 'id' and f'{col}_gaz' in final_merged_df.columns:\n",
        "        final_merged_df[col] = final_merged_df[f'{col}_gaz'].combine_first(final_merged_df[col])\n",
        "\n",
        "# Drop redundant '_gaz' columns\n",
        "final_merged_df = final_merged_df[merged_df.columns]\n",
        "\n",
        "# Save the final merged dataframe\n",
        "final_merged_df.to_csv('/content/drive/Shareddrives/EmDigIt/Processing/final_merge.csv', index=False)\n",
        "\n",
        "# -------------------------- STEP 4: CLEAN & ORGANIZE DATA ------------------------- #\n",
        "\n",
        "# Optional: reimport concat_lines for additional processing\n",
        "# concat_df = pd.read_csv(\"/content/drive/Shareddrives/EmDigIt/Processing/clean_df.csv\")\n",
        "\n",
        "# Sort final_df to maintain order\n",
        "final_df = final_merged_df.sort_values(['page', 'region', 'line']).drop_duplicates()\n",
        "\n",
        "# Assign route descriptions efficiently\n",
        "final_df['route_description'] = ''\n",
        "route_header_mask = final_df['line_type'] == 'route-header'\n",
        "final_df.loc[route_header_mask, 'route_description'] = final_df.loc[route_header_mask, 'content']\n",
        "final_df['route_description'].fillna(method='ffill', inplace=True)  # Fill down until 'sum-distance'\n",
        "\n",
        "# Save the final cleaned dataframe\n",
        "final_df.to_csv('/content/drive/Shareddrives/EmDigIt/Processing/final.csv', index=False)\n",
        "\n",
        "# -------------------------- STEP 5: UPLOAD TO GOOGLE SHEETS ---------------------- #\n",
        "\n",
        "# Authenticate Google Colab with user's credentials\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Create a new Google Sheets document\n",
        "sh = gc.create(itinerary_name)\n",
        "\n",
        "# File-to-Sheet Mapping\n",
        "file_mappings = {\n",
        "    'tidy_alto.csv': 'tidied alto',\n",
        "    'text_regions.csv': 'text regions',\n",
        "    'new_lines.csv': 'new lines',\n",
        "    'tagged_lines.csv': 'tagged',\n",
        "    'concat_lines.csv': 'concatenated',\n",
        "    'geolocated.csv': 'geolocated',\n",
        "    'gazetteer.csv': 'gazetteer',\n",
        "    'final.csv': 'final'\n",
        "}\n",
        "\n",
        "# Upload each CSV file to its corresponding Google Sheets tab\n",
        "for file_name, sheet_name in file_mappings.items():\n",
        "    file_path = os.path.join(\"/content/drive/Shareddrives/EmDigIt/Processing\", file_name)\n",
        "    if os.path.exists(file_path):  # Ensure file exists before attempting to read\n",
        "        df = pd.read_csv(file_path)\n",
        "        worksheet = sh.add_worksheet(title=sheet_name, rows=df.shape[0] + 1, cols=df.shape[1])\n",
        "        set_with_dataframe(worksheet, df)\n",
        "\n",
        "# Print Google Sheets link\n",
        "print(f\"Google Sheets document created: https://docs.google.com/spreadsheets/d/{sh.id}\")\n"
      ],
      "metadata": {
        "id": "E3m69BZSp08k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}